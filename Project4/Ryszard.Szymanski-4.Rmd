---
title: "Project 4"
author: "Ryszard Szymański"
date: "4/4/2020"
output: html_document
---

```{r setup, include=FALSE}
library(igraph)
library(ggplot2)
library(purrr)
knitr::opts_chunk$set(echo = TRUE)
```

## Zadanie 1
Do wykonania zadań wykorzystano sieć emaili firmy Enron (źródło: http://networkrepository.com/email-EU.php).
```{r}
hist2df <- function(hist_vals) {
  data.frame(
    breaks = hist_vals$breaks[seq_along(hist_vals$counts)],
    counts = hist_vals$counts
  )
}
enron <- drake::readd(enron_graph)
hist_vals <- hist(degree(enron), plot = FALSE, probability = TRUE)

hist_df <- hist2df(hist_vals)

ggplot(hist_df, aes(x = breaks, y = counts)) + 
  geom_line()
```

## Zadanie 2
Rysunek jest bardziej czytelny -- bliżej mu do linii prostej, której się spodziewamy przy rozkładach potęgowych. Można jednak zauważyć szum bliżej końca narysowanej linii.
```{r}
ggplot(hist_df, aes(x = breaks, y = counts)) + 
  geom_line() + 
  scale_x_log10() +
  scale_y_log10()
```

## Zadanie 3 i 4
Zastosowanie logarytmicznego binowania pozwoliło na wygładzenie szumu, które pojawiło się w zadaniu 2
```{r}
enron_degrees <- degree(enron)
breaks <- exp(seq(log(min(enron_degrees)), log(max(enron_degrees)), length.out = length(hist_vals$breaks)))
hist_vals_log_binned <- hist(enron_degrees, breaks = breaks, probability = TRUE)

hist_df <- hist2df(hist_vals_log_binned)

ggplot(hist_df, aes(x = breaks, y = counts)) + 
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```


## Zadanie 5
```{r}
enron_cdf <- ecdf(enron_degrees) 
enron_cdf_summary <- summary(enron_cdf)
cdf_min <- enron_cdf_summary[["Min."]]
cdf_max <- enron_cdf_summary[["Max."]]

x <- seq(cdf_min, cdf_max)
survival_fun_values <- 1 - enron_cdf(x)
data.frame(
  x = x,
  y = survival_fun_values
) %>% ggplot(aes(x = x, y = y)) +
  geom_line()
```

## Zadanie 6
Najbardziej czytelny jest wykres w którym zastosowano jednocześnie skale logarytmiczne oraz logarytmiczne binowanie.

## Zadanie 7
```{r}
lm_fit <- lm(log(counts) ~ log(breaks), data = hist_df)
lm_fit
```
## Zadanie 8
```{r}
alpha <- 1 + length(hist_vals_log_binned$counts) * sum(log(hist_vals_log_binned$counts / min(hist_vals_log_binned$counts)))^(-1)
alpha
```
## Zadanie 9
```{r}
alpha_no_xmin <- 1 + length(hist_vals_log_binned$counts) * sum(log(hist_vals_log_binned$counts))^(-1)
alpha - alpha_no_xmin
```

## Zadanie 12
```{r}
mean_nearest_neighbors_degree <- function(graph) {
  single_vertex_mean_nearest_neighbors_degree <- function(graph, v) {
    vertex_neighbors <- neighbors(graph, v, mode = "all") 
    mean(degree(graph = graph, v = vertex_neighbors))
  }
  
  sapply(V(graph), single_vertex_mean_nearest_neighbors_degree, graph = graph)
}

df <- data.frame(
  vertex_degree = degree(enron),
  mean_nearest_neighbors_degree = mean_nearest_neighbors_degree(enron)
)

ggplot(df, aes(x = vertex_degree, y = mean_nearest_neighbors_degree)) +
  geom_point() +
  labs(x = "Vertex degree", y = "Mean degree of nearest neighbors")
```
## Projekt
Wszystkie zaimplementowane miary zostały porównane z ich odpowiednikami z biblioteki `igraph` na następującym grafie:
```{r}
test_graph <- drake::readd(example_graph_project4)
plot(test_graph)
```

### Średni stopień najbliższego węzła
```{r}
mean_nearest_vertex_degree <- function(graph) {
  nearest_neighbors_count <- degree(graph) %>% sum()
  
  nearest_neighbors_degrees <- sapply(V(graph), function(vertex_ind) {
    neighbors(graph, vertex_ind) %>% 
      degree(graph, .) %>% 
      sum()
  })
  
  sum(nearest_neighbors_degrees) / nearest_neighbors_count
}
```

```{r}
mean_nearest_vertex_degree(test_graph)
```

### Wykładnik w sieciach potęgowych
```{r}
# source: http://konect.uni-koblenz.de/statistics/power#b408
network_exponent <- function(vertex_degrees, xmin = 1) {
  vertex_count <- length(vertex_degrees)
  
  alpha <- 1 + vertex_count * sum(log(vertex_degrees / xmin))^(-1)
  sigma <- (alpha - 1) / sqrt(vertex_count)
  
  list(
    alpha = alpha,
    sigma = sigma
  )
}

```

```{r}
sample_vertex_degrees <- poweRlaw::rpldis(1e6, xmin = 100, alpha = -2.5)
network_exponent(sample_vertex_degrees, xmin = 100)
```

### Współczynnik korelacji
```{r}
network_correlation <- function(graph) {
  edges_count <- ecount(graph)
  edges <- E(graph)
  adjacent_vertices <- ends(graph, edges)
  
  adjacent_vertices_product <- apply(adjacent_vertices, 1, function(v) {
    degree(graph, v[1]) * degree(graph, v[2])
  })
  adjacent_vertices_sum <- apply(adjacent_vertices, 1, function(v) {
    degree(graph, v[1]) + degree(graph, v[2])
  })
  adjacent_vertices_sum_of_squares <- apply(adjacent_vertices, 1, function(v) {
    degree(graph, v[1])^2 + degree(graph, v[2])^2
  })
  
  nominator_left <- sum(adjacent_vertices_product) / edges_count
  nominator_right <- (sum(adjacent_vertices_sum) / (2 * edges_count))^2
  denominator_left <- sum(adjacent_vertices_sum_of_squares) / (2*edges_count)
  denominator_right <- (sum(adjacent_vertices_sum)/(2*edges_count))^2
  
  (nominator_left - nominator_right) / (denominator_left - denominator_right)
}
```


```{r}
network_correlation(test_graph)
```

```{r}
igraph::assortativity_degree(test_graph)
```


### Współczynniki Gronowania

#### Wersja 1
```{r}
clustering_coefficient_1 <- function(graph) {
  node_clustering_coefficient <- function(graph, v) {
    v_degree <- degree(graph, v)
    v_neighbors <- neighbors(graph, v) %>% as.numeric()
    neighborhood_edges_count <- as_adjacency_matrix(graph)[v_neighbors, v_neighbors] %>% 
      sum() / 2
    possible_edges_count <- choose(v_degree, 2)
    
    clustering_coefficient <- neighborhood_edges_count / possible_edges_count
    if (is.nan(clustering_coefficient)) {
      NA
    } else {
      clustering_coefficient
    }
  }
  
  mean(sapply(V(graph), node_clustering_coefficient, graph = graph), na.rm = TRUE)
}

```

```{r}
clustering_coefficient_1(test_graph)
```

#### Wersja 2
```{r}
clustering_coefficient_2 <- function(graph) {
  triangles_count <- length(triangles(graph)) / 3
  all_simple_paths_of_legth_2_count <- do.call(c, lapply(V(graph), all_simple_paths, graph = graph)) %>% 
    keep(function(path) length(path) == 3) %>% 
    unique() %>% 
    length() / 2 # 1-2-3 is the same path as 3-2-1
  
  (3 * triangles_count) / all_simple_paths_of_legth_2_count
}
```

```{r}
clustering_coefficient_2(test_graph)
```


```{r}
igraph::transitivity(test_graph)
```

### Średnia odległość
```{r}
mean_distance <- function(graph) {
  vertex_count <- vcount(graph)
  vertex_distances <- distances(graph)
  diag(vertex_distances) <- 0 # d(i,j) where i != j
  sum(vertex_distances) / (vertex_count * (vertex_count - 1))
}
```

```{r}
mean_distance(test_graph)
```

```{r}
igraph::mean_distance(test_graph)
```

### Wydajność
```{r}
efficiency <- function(graph) {
  vertex_count <- vcount(graph)
  vertex_distances_inverted <- 1 / distances(graph)
  diag(vertex_distances_inverted) <- 0 # d(i,j) where i != j
  sum(vertex_distances_inverted) / (vertex_count * (vertex_count - 1))
}
```

```{r}
efficiency(test_graph)
```

```{r}
brainGraph::efficiency(test_graph, "global")
```

### Pośrednictwo węzłowe
```{r}
node_betweenness <- function(graph) {
  single_node_betweenness <- function(graph, v) {
    start_nodes <- setdiff(V(graph), v)
    val <- 0
    
    for (start_node in start_nodes) {
      end_nodes <- V(graph) %>% 
        keep(function(x) x > start_node) %>% 
        setdiff(v)
      
      for (end_node in end_nodes) {
        all_paths <- all_shortest_paths(graph = graph,
                                        from = start_node,
                                        to = end_node)
        all_paths_count <- length(all_paths$res)
        all_between_paths_count <- all_paths$res %>% 
          keep(function(path) v %in% path) %>% 
          length()
        
        val <- val + all_between_paths_count / all_paths_count
      }
    }
    
    val
  }
  
  vertex_count <- vcount(graph)
  node_betweenness <- sapply(V(graph), single_node_betweenness, graph = graph)
  (2 * node_betweenness) / ((vertex_count - 1) * (vertex_count - 2))
}
```

```{r}
node_betweenness(test_graph)
```

**Uwaga:** Implementacja tej miary w igraphie nie zawiera czynnika: $\frac{2}{(N - 1)(N - 2)}$.
Źródło: https://igraph.org/r/doc/betweenness.html
```{r}
vertex_count <- vcount(test_graph)
2 * igraph::betweenness(test_graph) / ((vertex_count - 1) * (vertex_count - 2))
```

